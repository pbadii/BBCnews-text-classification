{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4db0eabf",
   "metadata": {},
   "source": [
    "## Vectorization And Data Processing for Modeling\n",
    "\n",
    "Now that the text data has undergone basic cleaning and exploratory analysis, it's time to move on to preparing the data for modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d0771a",
   "metadata": {},
   "source": [
    "### Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f8fba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Core \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import joblib\n",
    "\n",
    "#-------- Text Feature extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "\n",
    "#-------- Dimensionality reduction / topic modeling\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "#-------- Preprocessing\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc541e7",
   "metadata": {},
   "source": [
    "### Import Cleaned Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5302231",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/interim/clean_train_df.csv')\n",
    "test_df = pd.read_csv('../data/interim/clean_test_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd78433a",
   "metadata": {},
   "source": [
    "## Data Processing \n",
    "\n",
    "### Text Normalization \n",
    "Now that the text data has some basic cleaning done, the next step is text normalization. With TF-IDF, text normalization is important because the same word will be considered different from its capitalized version (i.e. \"Dog\" and \"dog\" are considered 2 separate words). Since TF-IDF depends on word frequency, it is crucial that the same word is considered the same regardless of capitalization. This means the text must be normalized by changing the text to lowercase. \n",
    "\n",
    "#### Punctuation\n",
    "Additionally, it is worth considering changing punctuation rules ( ie only allowing . , - % \\$ )  as these are important for determining text content: business text will most definitely have $ and %, and in general, text can have words like \"mother-in-law\" where the '-' is important for differentiating from the words \"mother\" \"in\" and \"law\". However, other characters like # ( ) or * don't really weigh in on the content of the text though they affect tokenization, and thus can affect the models ability to classify. One other thing to look for is words with an ' in them such as possesive or contracted words (ie \"man's\" vs \"man s\" or \"it's\" vs \"it s\"). For TF-IDF, it doesn't really matter to have the apostrophe gone because changing the meaning of the word has no effect on the vectorization. Howevever, having the single character 's' multiple times will need to be dealt with to avoid inflating the vectorization due to the high frequency of that lone character. We can remove this lone 's' when we remove stop words. Thinking of other types of contraction like 've 'll 're 'd 'm 't -- let's remove them as well when cleaning stop words. most of these contractions, if expanded, would be stop words anyway {have, will, are, etc }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0452ca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    # Normalize a single text string\n",
    "        # Lowercase \n",
    "        # remove non-alphanumeric characters EXCEPT . , $ % and the unique token markers <>\n",
    "        # collapse multiple spaces into a single one \n",
    "            # Even though this was partially done in EDA, doing it here is important\n",
    "            # in the case that removing characters introduces extra space\n",
    "        # strip leading/trailing white space\n",
    "\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove unwanted characters //include <> in this exception to allow unique tokens <NUM> and <URL>\n",
    "    text = re.sub(r'[^a-z0-9\\.,$%<> ]+', '', text)\n",
    "\n",
    "    # collapse spaces and strip leading/trailing space\n",
    "    text = re.sub(r'\\s+',' ', text).strip()\n",
    "\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a33b74da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Normalized_text'] = train_df['Text'].apply(normalize_text)\n",
    "\n",
    "test_df['Normalized_text'] = test_df['Text'].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e7ed0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness. cynthia cooper worldcom s ex-head of internal accounting alerted directors to irregular accounting practices at the us telecoms giant in <NUM>. her warnings led to the collapse of the firm following the discovery of an $<NUM>bn (<NUM>bn) accounting fraud. mr ebbers has pleaded not guilty to charges of fraud and conspiracy. prosecution lawyers have argued that mr ebbers orchestrated a series of accounting tricks at worldcom ordering employees to hide expenses and inflate revenues to meet wall street earnings estimates. but ms cooper who now runs her own consulting business told a jury in new york on wednesday that external auditors arthur andersen had approved worldcom s accounting in early <NUM> and <NUM>. she said andersen had given a green light to the procedures and practices used by worldcom. mr ebber s lawyers have said he was unaware of the fraud arguing that auditors did not alert him to any problems. ms cooper also said that during shareholder meetings mr ebbers often passed over technical questions to the company s finance chief giving only brief answers himself. the prosecution s star witness former worldcom financial chief scott sullivan has said that mr ebbers ordered accounting adjustments at the firm telling him to hit our books . however ms cooper said mr sullivan had not mentioned anything uncomfortable about worldcom s accounting during a <NUM> audit committee meeting. mr ebbers could face a jail sentence of <NUM> years if convicted of all the charges he is facing. worldcom emerged from bankruptcy protection in <NUM> and is now known as mci. last week mci agreed to a buyout by verizon communications in a deal valued at $<NUM>bn.\n",
      "worldcom exboss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness. cynthia cooper worldcom s exhead of internal accounting alerted directors to irregular accounting practices at the us telecoms giant in <num>. her warnings led to the collapse of the firm following the discovery of an $<num>bn <num>bn accounting fraud. mr ebbers has pleaded not guilty to charges of fraud and conspiracy. prosecution lawyers have argued that mr ebbers orchestrated a series of accounting tricks at worldcom ordering employees to hide expenses and inflate revenues to meet wall street earnings estimates. but ms cooper who now runs her own consulting business told a jury in new york on wednesday that external auditors arthur andersen had approved worldcom s accounting in early <num> and <num>. she said andersen had given a green light to the procedures and practices used by worldcom. mr ebber s lawyers have said he was unaware of the fraud arguing that auditors did not alert him to any problems. ms cooper also said that during shareholder meetings mr ebbers often passed over technical questions to the company s finance chief giving only brief answers himself. the prosecution s star witness former worldcom financial chief scott sullivan has said that mr ebbers ordered accounting adjustments at the firm telling him to hit our books . however ms cooper said mr sullivan had not mentioned anything uncomfortable about worldcom s accounting during a <num> audit committee meeting. mr ebbers could face a jail sentence of <num> years if convicted of all the charges he is facing. worldcom emerged from bankruptcy protection in <num> and is now known as mci. last week mci agreed to a buyout by verizon communications in a deal valued at $<num>bn.\n"
     ]
    }
   ],
   "source": [
    "#Manually checking one example to compare text and normalized text result\n",
    "temp1 = train_df['Text'][0]\n",
    "temp2 = train_df['Normalized_text'][0]\n",
    "print(temp1)\n",
    "print(temp2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce5132b",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "unigram and bigram are the combinations of words that will be tokenized (every word or combination of 2 words) which is standard practice. \n",
    "The primary purpose of sublinear_tf is to prevent terms that appear an extremely high number of times in a single document from dominating the vector representation, which is a different issue from the bias caused by the overall length of the document\n",
    "\n",
    "#### Lemmatization\n",
    "lemmatization usually adds value because:\n",
    "* It reduces different word forms to their base form (e.g., running → run, better → good).\n",
    "* That helps models generalize better since the same concept isn’t split into multiple features.\n",
    "* Especially important for smaller corpora like BBC articles, where data sparsity can otherwise hurt clustering or classification.\n",
    "\n",
    "One caveat: lemmatization adds processing time, especially if we use spaCy (faster but heavier) or NLTK (lighter but slower). spaCy is industry standard, but worth considering disabling NER and parse which are features in spaCy that are not needed in this project and increase computation.\n",
    "If our goal is document classification (like BBC news topic classification)\n",
    "\n",
    "NER is usually not needed because:\n",
    "* Entities like “Google” or “New York” will already appear as tokens in the text.\n",
    "* TF–IDF, CountVectorizer, or embeddings capture them directly.\n",
    "* NER would just add computational overhead without giving extra features unless we explicitly extract the entities and use them as features.\n",
    "\n",
    "If our goal is entity-aware tasks (like information extraction, knowledge graphs, search engines, or summarization) then yes — keeping NER would be very useful, because we’d want to know which tokens are entities, and potentially treat them differently.\n",
    "\n",
    "Standard practice in industry text classification pipelines\n",
    "* Don’t run NER or parser unless they directly contribute to your features.\n",
    "* For classification, lemmatization + stopwords + vectorization is usually enough.\n",
    "\n",
    "So, for our current BBC classification project, the lean pipeline (disable=[\"parser\",\"ner\"]) is the best practice.\n",
    "\n",
    "#### Stopword removal\n",
    "need custom list in addition to english stopwrods to account for contraction errors when we removed apostrophe in punctuation normalization. We can pull the english stop words from the sklearn.feature_Extraction.Text ENGLISH_STOP_WORDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93e56992",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_.lower() for token in doc if token.is_alpha])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36cbcb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Norm_Lemma_text'] = train_df['Normalized_text'].apply(lemmatize_text)\n",
    "test_df['Norm_Lemma_text'] = test_df['Normalized_text'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca240c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worldcom exboss launch defence lawyer defend former worldcom chief bernie ebber against a battery of fraud charge have call a company whistleblow as their first witness cynthia cooper worldcom s exhead of internal accounting alert director to irregular accounting practice at the us telecom giant in num her warning lead to the collapse of the firm follow the discovery of an num bn num bn accounting fraud mr ebber have plead not guilty to charge of fraud and conspiracy prosecution lawyer have argue that mr ebber orchestrate a series of accounting trick at worldcom order employee to hide expense and inflate revenue to meet wall street earning estimate but ms cooper who now run her own consulting business tell a jury in new york on wednesday that external auditor arthur andersen have approve worldcom s account in early num and num she say andersen have give a green light to the procedure and practice use by worldcom mr ebber s lawyer have say he be unaware of the fraud argue that auditor do not alert he to any problem ms cooper also say that during shareholder meeting mr ebber often pass over technical question to the company s finance chief give only brief answer himself the prosecution s star witness former worldcom financial chief scott sullivan have say that mr ebber order accounting adjustment at the firm tell he to hit our book however ms cooper say mr sullivan have not mention anything uncomfortable about worldcom s account during a num audit committee meeting mr ebber could face a jail sentence of num year if convict of all the charge he be face worldcom emerge from bankruptcy protection in num and be now know as mci last week mci agree to a buyout by verizon communication in a deal value at num bn\n"
     ]
    }
   ],
   "source": [
    "#Take a look\n",
    "print(train_df['Norm_Lemma_text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca10a689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Stop words\n",
    "english_stopWords = list(ENGLISH_STOP_WORDS)\n",
    "contractions = [\"s\", \"t\", \"ve\",\"re\", \"d\", \"ll\", \"m\"]\n",
    "custom_stopWords = english_stopWords + contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5670976e",
   "metadata": {},
   "source": [
    "#### Splitting Data \n",
    "\n",
    "Since I only have labels for the training dataset and not the testing dataset, I have to split the training dataset 80-20 (as is convention) so I can train the model on 80% of the train data, then evaluate the model using the remaining 20% portion and the corresponding labels. After training the model, I can predict labels for the full Testing dataset that does not have labels. \n",
    "\n",
    "This split needs to happen once the data is cleaned, normalized, and lemmatized but before vectorizing and doing dimensionality reduction. Splitting after vectorization will result in matrices that are biased because the decomposition would have \"seen\" all the labeled data (20% of which we would use to evaluate the model, hence the model is biased)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a99bf93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "      <th>Word_count</th>\n",
       "      <th>text_length</th>\n",
       "      <th>Normalized_text</th>\n",
       "      <th>Norm_Lemma_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1833</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "      <td>301</td>\n",
       "      <td>1856</td>\n",
       "      <td>worldcom exboss launches defence lawyers defen...</td>\n",
       "      <td>worldcom exboss launch defence lawyer defend f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>154</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "      <td>325</td>\n",
       "      <td>2009</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>german business confidence slide german busine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1101</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "      <td>514</td>\n",
       "      <td>3132</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>bbc poll indicate economic gloom citizen in a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1976</td>\n",
       "      <td>lifestyle governs mobile choice faster better ...</td>\n",
       "      <td>tech</td>\n",
       "      <td>634</td>\n",
       "      <td>3583</td>\n",
       "      <td>lifestyle governs mobile choice faster better ...</td>\n",
       "      <td>lifestyle govern mobile choice fast well or fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>917</td>\n",
       "      <td>enron bosses in $&lt;NUM&gt;m payout eighteen former...</td>\n",
       "      <td>business</td>\n",
       "      <td>355</td>\n",
       "      <td>2189</td>\n",
       "      <td>enron bosses in $&lt;num&gt;m payout eighteen former...</td>\n",
       "      <td>enron boss in num m payout eighteen former enr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ArticleId                                               Text  \\\n",
       "0           0       1833  worldcom ex-boss launches defence lawyers defe...   \n",
       "1           1        154  german business confidence slides german busin...   \n",
       "2           2       1101  bbc poll indicates economic gloom citizens in ...   \n",
       "3           3       1976  lifestyle governs mobile choice faster better ...   \n",
       "4           4        917  enron bosses in $<NUM>m payout eighteen former...   \n",
       "\n",
       "   Category  Word_count  text_length  \\\n",
       "0  business         301         1856   \n",
       "1  business         325         2009   \n",
       "2  business         514         3132   \n",
       "3      tech         634         3583   \n",
       "4  business         355         2189   \n",
       "\n",
       "                                     Normalized_text  \\\n",
       "0  worldcom exboss launches defence lawyers defen...   \n",
       "1  german business confidence slides german busin...   \n",
       "2  bbc poll indicates economic gloom citizens in ...   \n",
       "3  lifestyle governs mobile choice faster better ...   \n",
       "4  enron bosses in $<num>m payout eighteen former...   \n",
       "\n",
       "                                     Norm_Lemma_text  \n",
       "0  worldcom exboss launch defence lawyer defend f...  \n",
       "1  german business confidence slide german busine...  \n",
       "2  bbc poll indicate economic gloom citizen in a ...  \n",
       "3  lifestyle govern mobile choice fast well or fu...  \n",
       "4  enron boss in num m payout eighteen former enr...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#review train_df\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3249f66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_df['Norm_Lemma_text']\n",
    "train_labels = train_df['Category']\n",
    "\n",
    "test_text = test_df['Norm_Lemma_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65b7cafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    worldcom exboss launch defence lawyer defend f...\n",
       "1    german business confidence slide german busine...\n",
       "2    bbc poll indicate economic gloom citizen in a ...\n",
       "3    lifestyle govern mobile choice fast well or fu...\n",
       "4    enron boss in num m payout eighteen former enr...\n",
       "Name: Norm_Lemma_text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a518a0f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    business\n",
       "1    business\n",
       "2    business\n",
       "3        tech\n",
       "4    business\n",
       "Name: Category, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90fcf6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "# Stratify ensures label balance across train and validation sets\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_text, train_labels, test_size=0.2, random_state=88, stratify=train_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d30090",
   "metadata": {},
   "source": [
    "#### Vectorization with TF-IDF VEctorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4968b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/tfidf_vectorizer.joblib']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constructing TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    lowercase= False,   #already done\n",
    "    max_features=10000, #only returns the top 10000 features to reduce computation costs and potential noise\n",
    "    stop_words= custom_stopWords,\n",
    "    ngram_range=(1,2),  #unigrams and bigrams \n",
    "    min_df= 2,      #Remove very rare occurance tokens\n",
    "    max_df= 0.95,   #remove high frequency tokens\n",
    "    sublinear_tf=True   #Reduce effect of extremely frequent tokens\n",
    ")\n",
    "\n",
    "#fit and transform training and validation sets\n",
    "tfidf_X_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "tfidf_X_val = tfidf_vectorizer.transform(X_val)\n",
    "\n",
    "\n",
    "#transform test set //test set must use the same vocabulary and IDF weights learned from the training set.\n",
    "tfidf_test = tfidf_vectorizer.transform(test_text) #Don't use fit_transform, just transform\n",
    "\n",
    "#For reproducibility, export the vectorizer\n",
    "joblib.dump(tfidf_vectorizer, \"../models/tfidf_vectorizer.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f71c5e",
   "metadata": {},
   "source": [
    "#### Vectorization with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "076c860d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/count_vectorizer.joblib']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constructing the CountVectorizer \n",
    "count_vectorizer = CountVectorizer(\n",
    "    lowercase=False,    #already done\n",
    "    stop_words=custom_stopWords,\n",
    "    ngram_range=(1,2),\n",
    "    min_df=2,\n",
    "    max_df=0.95, \n",
    "    max_features=10000\n",
    ")\n",
    "# fit and transform training and validation sets\n",
    "count_X_train = count_vectorizer.fit_transform(X_train)\n",
    "count_X_val = count_vectorizer.transform(X_val)\n",
    "\n",
    "count_test = count_vectorizer.transform(test_text)\n",
    "\n",
    "#For reproducibility\n",
    "joblib.dump(count_vectorizer, \"../models/count_vectorizer.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71859cd7",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction and Normalization\n",
    "\n",
    "Using Truncated SVD (Not PCA because PCA creates a dense matrix but the tfidf and the count result in a sparse matrix which truncatedSVD can handle just fine) then using normalizer with L2 ensures each document vector has a unit length, which is beneficial because the dot product of two normalized vectors equals their cosine similarity—a standard metric for measuring document similarity in information retrieval.\n",
    "\n",
    "reusing the same lsa_pipeline object for both TF-IDF and Count vectors. That’s a subtle but important issue that when you call the pipeline with TFIDF data to fit transform and then use the same pipeline on the COUNT data, then you rewrite the fitted transform. So, create two pipelines so each can hold the SVD matrix for their corresponding vectorized data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94ad8c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/count_lsa.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create LSA (latent sentiment analysis) pipeline to preprocess data uniformly\n",
    "# uses Truncated SVD and Normalizer with L2\n",
    "\n",
    "tfidf_lsa = make_pipeline(\n",
    "    TruncatedSVD(n_components=100, random_state=88),\n",
    "    Normalizer(copy = False)\n",
    ")\n",
    "\n",
    "count_lsa = make_pipeline(\n",
    "    TruncatedSVD(n_components=100, random_state=88),\n",
    "    Normalizer(copy = False)\n",
    ")\n",
    "joblib.dump(tfidf_lsa, \"../models/tfidf_lsa.pkl\")\n",
    "joblib.dump(count_lsa, \"../models/count_lsa.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfd0fa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce matrices and plot\n",
    "tfidf_X_train_reduced = tfidf_lsa.fit_transform(tfidf_X_train)\n",
    "tfidf_X_val_reduced = tfidf_lsa.transform(tfidf_X_val)\n",
    "tfidf_test_reduced = tfidf_lsa.transform(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b121f403",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_X_train_reduced = count_lsa.fit_transform(count_X_train)\n",
    "count_X_val_reduced = count_lsa.transform(count_X_val)\n",
    "count_test_reduced = count_lsa.transform(count_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce65d02e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/processed/count_test_reduced.pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export data for modeling\n",
    "\n",
    "#TFIDF reduced matrices\n",
    "joblib.dump(tfidf_X_train_reduced,\"../data/processed/tfidf_X_train_reduced.pkl\")\n",
    "joblib.dump(tfidf_X_val_reduced, \"../data/processed/tfidf_X_val_reduced.pkl\")\n",
    "joblib.dump(tfidf_test_reduced, \"../data/processed/tfidf_test_reduced.pkl\") \n",
    "\n",
    "#Count reduced matrices\n",
    "joblib.dump(count_X_train_reduced, \"../data/processed/count_X_train_reduced.pkl\")\n",
    "joblib.dump(count_X_val_reduced, \"../data/processed/count_X_val_reduced.pkl\")\n",
    "joblib.dump(count_test_reduced, \"../data/processed/count_test_reduced.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
